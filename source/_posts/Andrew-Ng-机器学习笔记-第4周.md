---
title: Andrew Ng 机器学习笔记(第4周)
math: true
date: 2022-06-03 17:27:15
categories: 人工智能
excerpt: 机器学习课程第4周的笔记，本周的内容比较少，介绍了最基础的神经网络及其应用，本文篇幅也较短。
index_img: /img/AI/ai_index_4.jpg
banner_img: /img/AI/ai_banner_4.jpg
---

## 前言

这个系列的博客将作为我机器学习课程的笔记，部分用中文较难表述的词句我将使用英文原文。

第 4 周的课程标题是神经网络的表示，介绍了最基础的神经网络及其应用，本周的内容比较少，仅涉及到**全连接层**和**激活函数层**。

## 神经网络

**引入神经网络的动机：**当输入数据(这里仅考虑向量)的维度较大时，很难给出待定函数的表达式，因为表达式的最高次项的阶数难以确定。于是，研究者们考虑使用具有固定数量参数的神经网络，来学习目标函数。

**神经网络的表示：**记$g$为$sigmoid$函数，$\Theta^{(i)}$为第$i$层的参数矩阵，$a^{(i)}$为第$i$层的输入向量，则有：

$$
a^{(i+1)}=g\left(\Theta^{(i)}a^{(i)}\right)
$$

在上式中，$a_0^{(i)}$(第$i$层输入向量的第$1$个分量)恒等于$1$，故这里不需要再加上偏置向量。

## 神经网络的简单应用

**本节第一部分**展示了使用神经网络完成与门、或门、异或门的功能，网络非常简单，在此不再列出。

**本节第二部分**给出了使用神经网络解决多类分类问题的一个例子，粗略讲解了一个图像分类的神经网络。

## 相关链接

- [Andrew Ng 机器学习课程](https://www.coursera.org/learn/machine-learning)
- [理解图神经网络：从 CNN 到 GNN](https://zhuanlan.zhihu.com/p/463666907)
- [一文搞懂 RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)
- [深度学习中 Attention 与全连接层的区别何在](https://www.zhihu.com/question/320174043/answer/651998472)

由于作者水平有限，所以文章中难免有少数不严谨之处，如有读者发现此类疏忽，恳请读者指出。另外，如果认为本文对您有帮助，欢迎请作者喝咖啡！![Matrix53的微信赞赏码](/img/global/wxQRcode_pay.png)
